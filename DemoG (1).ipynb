{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV4PlNOxVTij"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dETxgGZ1Vn0V"
      },
      "outputs": [],
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U \"keras>=3\"\n",
        "!pip install -U keras-hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcjAY3XoVugm"
      },
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "# Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svaN6YzXVz_a"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBn-AbwEp9YJ",
        "outputId": "d74a0ed2-1050-43e3-cd69-a7bc8bb6c38d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-08 08:11:09--  https://huggingface.co/datasets/Vineeth11/Equipd/resolve/main/yoto.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.23, 18.164.174.118, 18.164.174.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33317 (33K) [text/plain]\n",
            "Saving to: ‘yoto.jsonl’\n",
            "\n",
            "yoto.jsonl          100%[===================>]  32.54K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-01-08 08:11:10 (4.33 MB/s) - ‘yoto.jsonl’ saved [33317/33317]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O yoto.jsonl https://huggingface.co/datasets/Vineeth11/Equipd/resolve/main/yoto.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UetYafN8qM8B"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "data = []\n",
        "with open(\"yoto.jsonl\") as file:\n",
        "    for line in file:\n",
        "        features = json.loads(line)\n",
        "        # Filter out examples with context, to keep it simple.\n",
        "        if features[\"context\"]:\n",
        "            continue\n",
        "        # Format the entire example as a single string.\n",
        "import json\n",
        "data = []\n",
        "with open(\"yoto.jsonl\") as file:\n",
        "    for line in file:\n",
        "        features = json.loads(line)\n",
        "        # Filter out examples with context, to keep it simple.\n",
        "        if features[\"context\"]:\n",
        "            continue\n",
        "        # Format the entire example as a single string.\n",
        "        try:\n",
        "            template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "            data.append(template.format(**features))\n",
        "        except KeyError:\n",
        "            # Handle cases where 'response' is missing (e.g., skip or print a warning)\n",
        "            print(f\"Warning: 'response' key missing in line: {line.strip()}\")\n",
        "\n",
        "\n",
        "# Only use 100 training examples, to keep it fast.\n",
        "data = data[:100]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKxPUsZJXWPu"
      },
      "outputs": [],
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uejFGI-GZm2j"
      },
      "outputs": [],
      "source": [
        "# Define the template outside the with block to make it accessible\n",
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction=\"What is Machine Learning?\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aVcKBX0WZaj"
      },
      "outputs": [],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What are the types of classification?\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V8YBLATWegb"
      },
      "outputs": [],
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 4.\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-7eYPAGNWwZa"
      },
      "outputs": [],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What is Deep Learning?.\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=300))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Peq7TnLtHse"
      },
      "outputs": [],
      "source": [
        "# Limit the input sequence length to 256 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 256\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "gemma_lm.fit(data, epochs=1, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZyC8rOAArry"
      },
      "outputs": [],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What is Deep Learning?\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PUnLFNaBBNn"
      },
      "outputs": [],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What is Deep Learning?\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Pkh_-8wr41lM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Example data (replace with your actual metrics)\n",
        "epochs = np.arange(1, 101)  # Epochs from 1 to 100\n",
        "train_acc = np.linspace(0.1, 0.9, 100)  # Example training accuracy\n",
        "val_acc = np.linspace(0.15, 0.85, 100)  # Example validation accuracy\n",
        "\n",
        "train_loss = np.linspace(3.5, 2.5, 100) + np.random.randn(100) * 0.1  # Example training loss\n",
        "val_loss = np.linspace(3.6, 2.6, 100) + np.random.randn(100) * 0.1  # Example validation loss\n",
        "\n",
        "train_grad_norm = np.linspace(9, 4, 100) + np.random.randn(100) * 0.5  # Example gradient norm (train)\n",
        "val_grad_norm = np.linspace(9, 4.2, 100) + np.random.randn(100) * 0.5  # Example gradient norm (val)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(epochs, train_acc, label=\"training\", color=\"purple\", linewidth=2)\n",
        "plt.plot(epochs, val_acc, label=\"validation\", color=\"violet\", linewidth=2, alpha=0.7)\n",
        "plt.title(\"Gemma 2B Model Training and Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Plot Gradient Norm\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(epochs, train_grad_norm, label=\"training\", color=\"blue\", linewidth=2)\n",
        "plt.plot(epochs, val_grad_norm, label=\"validation\", color=\"lightblue\", linewidth=2, alpha=0.7)\n",
        "plt.title(\"Gemma 2B Model Gradient Norm Training and Validation\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"grad_norm\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(epochs, train_loss, label=\"training\", color=\"orange\", linewidth=2)\n",
        "plt.plot(epochs, val_loss, label=\"validation\", color=\"gold\", linewidth=2, alpha=0.7)\n",
        "plt.title(\"Gemma 2B Model Training and Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wRrEwEGi_NWq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data for the plot\n",
        "epochs = np.linspace(0, 100, 100)  # x-axis (0 to 100 epochs)\n",
        "learning_rate = np.full_like(epochs, 2e-4)  # y-axis (constant learning rate 2e-4)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, learning_rate, label='training', color='limegreen')\n",
        "plt.scatter(epochs[0], learning_rate[0], color='limegreen')  # Highlight the start point\n",
        "\n",
        "# Title and labels\n",
        "plt.title(\"Gemma 2B Model Learning Rate Training\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Epochs\", fontsize=12)\n",
        "plt.ylabel(\"Learning Rate\", fontsize=12)\n",
        "\n",
        "# Customizing the axes\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.ylim(1.5e-4, 2.5e-4)  # Compressing the y-axis to fit within specific coordinates\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Adding a legend\n",
        "plt.legend(loc='upper right', fontsize=10, frameon=True, title_fontsize='12')\n",
        "\n",
        "# Arrow annotations\n",
        "plt.annotate('', xy=(10, 2e-4), xytext=(10, 2.4e-4),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='wedge,tail_width=0.7'))\n",
        "plt.text(5, 2.45e-4, 'Learning Rate', fontsize=10, rotation=90)\n",
        "plt.annotate('', xy=(90, 1.6e-4), xytext=(110, 1.6e-4),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='wedge,tail_width=0.7'))\n",
        "plt.text(95, 1.55e-4, 'Epochs', fontsize=10)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PClVrc_SkFhn"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "import gradio as gr\n",
        "\n",
        "# Placeholder for the `queries` function\n",
        "def queries(message):\n",
        "    return f\"Response to: {message}\"\n",
        "\n",
        "# Build Gradio App\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Welcome to Fine-Tuned LoRA Application with Gemma 2B!\n",
        "        Start interacting with your chatbot companion.\n",
        "        \"\"\"\n",
        "    )\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message here...\")\n",
        "\n",
        "    # Clear button\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    # Respond function\n",
        "    def respond(message, chat_history):\n",
        "        bot_message = queries(message)\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    # Clear chat function\n",
        "    def clear_chat():\n",
        "        return [], \"\"\n",
        "\n",
        "    # Event handling\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "    clear.click(clear_chat, [], [chatbot, msg])\n",
        "\n",
        "# Launch Gradio App\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "K1o23spFBpCN"
      },
      "outputs": [],
      "source": [
        "# gemma_lm.backbone.save(\"gemma_model.keras\")  # Change the filepath to include .keras extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FoL1Lk2C7lP"
      },
      "outputs": [],
      "source": [
        "#!pip install streamlit -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ_KajNCF0bv"
      },
      "source": [
        "write the cell python code into an app.py file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W9LobjhDDID"
      },
      "outputs": [],
      "source": [
        "#%%writefile app.py\n",
        "#import streamlit as st\n",
        "#from typing import List, Tuple\n",
        "#import keras\n",
        "\n",
        "#import keras_nlp\n",
        "\n",
        "# import subprocess\n",
        "# import sys\n",
        "\n",
        "# # Function to install a library\n",
        "# def install_library(library_name):\n",
        "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", library_name])\n",
        "\n",
        "# # Example: Install `keras-nlp` and `streamlit`\n",
        "# install_library(\"keras-nlp\")\n",
        "# # Load the backbone of the GEMMA model\n",
        "\n",
        "#from keras.models import load_model\n",
        "\n",
        "#gemma_lm_backbone = load_model(\"gemma_model.keras\")\n",
        "\n",
        "# Rebuild the GEMMA model with the loaded backbone\n",
        "#gemma_lm = keras_nlp.models.GemmaCausalLM(backbone=gemma_lm_backbone)\n",
        "\n",
        "# Configure the model (e.g., sampler, preprocessor settings) if needed\n",
        "#sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "#gemma_lm.compile(sampler=sampler)\n",
        "#gemma_lm.preprocessor.sequence_length = 256\n",
        "\n",
        "\n",
        "# Define a function for generating responses (mockup of the \"queries\" function)\n",
        "#def queries(Instruction: str) -> str:\n",
        "#    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "#    prompt = template.format(\n",
        "#      instruction=Instruction,\n",
        "#      response=\"\",\n",
        "#    )\n",
        "\n",
        " #   return f\"Response to: {gemma_lm.generate(prompt, max_length=256)}\"\n",
        "\n",
        "# Define a function to manage chatbot interactions\n",
        "#def respond(Instruction: str, chat_history: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
        "#    Response = queries(Instruction)\n",
        "#    chat_history.append((Instruction, Response))\n",
        "#    return chat_history\n",
        "\n",
        "# Streamlit UI|\n",
        "#st.set_page_config(page_title=\"Fine-Tuned LoRA Application with Gemma 2B\", layout=\"wide\")\n",
        "\n",
        "#st.markdown(\"\"\"\n",
        "    # Welcome to Fine-Tuned LoRA Application with Gemma 2B!\n",
        "#   Start interacting with your chatbot companion.\n",
        "#\"\"\")\n",
        "\n",
        "# Initialize chat history in session state\n",
        "#if \"chat_history\" not in st.session_state:\n",
        "#    st.session_state.chat_history = []\n",
        "\n",
        "# Chatbot UI\n",
        "#for Instruction, Response in st.session_state.chat_history:\n",
        "#    st.markdown(f\"**User:** {Instruction}\")\n",
        "#    st.markdown(f\"**Gemma 2B:** {Response}\")\n",
        "\n",
        "# User input\n",
        "#Instruction = st.text_input(\"Ask your question:\", key=\"Instruction\")\n",
        "#if st.button(\"Send\") or st.session_state.get(\"Response\"):\n",
        "#    if Instruction.strip():\n",
        "#        st.session_state.chat_history = respond(Instruction, st.session_state.chat_history)\n",
        "\n",
        "\n",
        "# Clear button\n",
        "#if st.button(\"Clear Chat\"):\n",
        "#    st.session_state.chat_history = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BOm-OgtfPMI"
      },
      "outputs": [],
      "source": [
        "#!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrB-GXUr7A1K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}